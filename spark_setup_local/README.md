â­  spark-local-setup  â­

# ğŸš€ Spark Local Setup using Docker & Jupyter

This guide helps you set up **Apache Spark** locally using **Docker Compose** and access it via **JupyterLab** for development and testing.

---

## ğŸ“Œ Prerequisites

- **Docker Desktop** installed and running
  ğŸ‘‰ Download from: https://www.docker.com/products/docker-desktop/

- Windows (PowerShell) or macOS/Linux (Terminal)

---

## ğŸ“‚ Project Structure

Create a folder named **`spark-setup`** at any location on your system and set up the following directory structure:

â”œâ”€â”€ docker-compose.yml

â”œâ”€â”€ jupyter/
       
       â””â”€â”€ Dockerfile
       
â”œâ”€â”€ notebooks/
       
â”œâ”€â”€event-logs/

â”œâ”€â”€spark-conf/
       
       â””â”€â”€ spark-defaults.conf


## â–¶ï¸ Starting the Spark Cluster

1. Open **Windows PowerShell** or **Mac/Linux Terminal**
2. Navigate to the `spark-setup` directory:

cd path/to/spark-setup


## â–¶ï¸ Build containers (One time only)

    command: docker compose build

## â–¶ï¸ Start the Spark cluster in detached mode:

    command: docker compose up -d

ğŸŒ Spark Web UI : Once the cluster is up, you can access the Spark UIs in your browser:

    Jupyter Lab â†’ http://localhost:8888/

    Driver UI â†’ http://localhost:4040

    Spark Master UI â†’ http://localhost:8080

    Worker 1 UI â†’ http://localhost:8081

    Worker 2 UI â†’ http://localhost:8082

â¹ï¸ Stopping the Cluster

    command: docker compose down

## â–¶ï¸ start again when needed

    command: docker compose up -d

ğŸ““ Verifying Spark in JupyterLab

Open JupyterLab from your browser (as configured in Docker).
Create or open a notebook.
Run the following PySpark code:

"""
    
    from pyspark.sql import SparkSession
    spark = (
        SparkSession.builder
        .master("spark://spark-master:7077")
        .appName("spark-app")
        .getOrCreate()
    )

    spark

"""

âœ… If successful, the output will display the Spark version and session details, confirming that Spark is running correctly.
ğŸ¯ You're All Set!

You now have a fully functional local Spark environment with Docker and JupyterLab.
Happy Spark-ing! ğŸ”¥


